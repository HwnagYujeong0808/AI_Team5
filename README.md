# AI_Team5
This is a GitHub repository for a team project in an Artificial Intelligence class.
+ **Google Drive containing all our code files, datasets, and models**: https://drive.google.com/drive/folders/15xNLqFqYo6bK5Jg3ehTNBYbANsL_2Cu8?usp=drive_link

## Motivation for the project and an explanation of the problem statement
> Motivation for the project

With the recent advancement of online video services and social media, there is a growing demand for 'short-form content' that can be consumed in a brief period.

![image](https://github.com/HwnagYujeong0808/AI_Team5/assets/66208800/5131ec68-85ee-438f-a1a5-971227d6ad88)

Table 1, displaying the results of a survey conducted on 5,000 individuals aged 15 to 59 in South Korea, illustrates the increase in the usage of short-form content over the past year. As indicated in Table 1, the number of users who watched short-form content has increased by more than 10% in the last year.


> Problem Definition

In today's society, where a large amount of information is generated and shared, content consumers have become accustomed to videos that are less than one minute long. There is a tendency to skip through long videos without watching them to the end and move on to the next one.
From the perspective of video content creators, manually editing long videos which are the source of short-form content, to extract highlight is highly inefficient and demands a significant amount of time and resources.


## A description of the data

- **Data 1: ETRI Korean Emotion Dataset - KEMDy20 (General Public Free Speech) Dataset**

  - **Link**: [KEMDy20\_Dataset](https://nanum.etri.re.kr/share/kjnoh/KEMDy20?lang=ko_KR)
  - **Introduction**: A multimodal emotion dataset collected for the analysis of the correlation between emotional speech, contextual meaning of speech, and physiological response signals such as skin conductance, heart rate-related data, and wrist skin temperature.
  - **Train set**
    - Download path: '01.데이터/2.Validation/원천데이터/VS\_유튜브\_04'
    - Only the video data within the smallest 21.4GB folder is used.
  - **Test set**
    - Download path: '01.데이터/2.Validation/원천데이터/VS\_유튜브\_01'
    - Only 8 video data within the folder are used
      - '유튜브*기타\_19843', '유튜브*반려동물및동물*2153', '유튜브*스타일링및뷰티*14630', '유튜브*스포츠*4174', '유튜브*여행*7640', '유튜브*음식*17341', '유튜브*일상*10479', '유튜브*자동차\_0094'

- **Data 2: AI HUB Video Content Highlight Editing and Description (Summarization) Data**
  - **Link**: [AIHUB\_Dataset](https://www.aihub.or.kr/aihubdata/data/view.do?dataSetSn=616)
  - **Information**: The AI HUB dataset on video content highlight editing and description (summarization) is a training dataset constructed by labeling the positions of key scenes in news and YouTube videos and tagging them for category items

## Hyperparameter and architecture choices that were explored
![image](https://github.com/HwnagYujeong0808/AI_Team5/assets/66208800/9d4490a1-ce43-4353-8476-01dccbbe76d4)


## Presentation of results

## Analysis of results

## Any insights and discussions relevant to the project

## References

## Extra credit

### Member's contribution statement

+ Please describe each member's contribution in detail. Blind peer review will be conducted after the final presentation.


### The Github repository with the commit history
